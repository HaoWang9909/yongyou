import streamlit as st
import pandas as pd
import numpy as np
import time
import plotly.graph_objs as go
from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor

def data_process(df, time_column, part_column, sales_column):
    #ä»…å–å‡ºæ—¶é—´ã€é›¶ä»¶ã€é”€é‡ä¸‰åˆ—
    df = df[[part_column, sales_column, time_column]]

    # å°†æ—¶é—´åˆ—è½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼
    df[time_column] = pd.to_datetime(df[time_column])

    # å°†é›¶ä»¶åˆ—è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
    df[part_column] = df[part_column].astype(str)

    # å°†é”€é‡åˆ—è½¬æ¢ä¸ºæµ®ç‚¹æ•°æ ¼å¼
    df[sales_column] = df[sales_column].astype(float)
    
    # é’ˆå¯¹æ¯ä¸ªé›¶ä»¶ç¼–å·ï¼ˆ'PART_NO'ï¼‰è¿›è¡Œåˆ†ç»„
    grouped = df.groupby(part_column)

    # åˆ›å»ºä¸€ä¸ªæ–°çš„DataFrameæ¥å­˜å‚¨å¤„ç†åçš„æ•°æ®
    capped_df = pd.DataFrame()

    for part_no, group in grouped:
        Q1 = group[sales_column].quantile(0.25)
        Q3 = group[sales_column].quantile(0.75)
        IQR = Q3 - Q1
        upper_limit = Q3 + 1.5 * IQR
        # å°†è¿‡å¤§å€¼æ›¿æ¢ä¸ºä¸Šé™
        group.loc[group[sales_column] > upper_limit, sales_column] = upper_limit
        # å°†å°äº0çš„å€¼æ›¿æ¢ä¸º0
        group.loc[group[sales_column] < 0, sales_column] = 0
        # å°†å¤„ç†åçš„æ•°æ®æ·»åŠ åˆ°capped_df
        capped_df = pd.concat([capped_df, group])
    # é‡ç½®ç´¢å¼•
    capped_df.reset_index(drop=True, inplace=True)
    # è¿”å›å¤„ç†åçš„æ•°æ®
    return capped_df


def main_predict(all_data,time_column,part_column,sales_column,part_to_predict,days_to_predict,predict_time):
    # å°†CREATE_DATEè½¬æ¢ä¸ºDateTimeæ•°æ®ç±»å‹
    all_data[time_column] = pd.to_datetime(all_data[time_column])
    ## è®¡ç®—æ¯ä¸ªé…ä»¶åœ¨æ¯ä¸€å¤©çš„é”€é‡
    daily_sales = all_data.groupby([part_column, pd.Grouper(key=time_column, freq='D')])[sales_column].sum().reset_index()
    # æ‰¾å‡ºæœ€æ—©å’Œæœ€æ™šçš„æ—¥æœŸ
    start_date = daily_sales[time_column].min()
    end_date = daily_sales[time_column].max()
    # åˆ›å»ºæ—¥æœŸèŒƒå›´
    all_days = pd.date_range(start_date, end_date, freq='D')
    def process_group(group):
        part_no = group[part_column].iloc[0]  # ä»åˆ†ç»„æ•°æ®ä¸­è·å– part_column
        group.set_index(time_column, inplace=True)
        group = group.reindex(all_days, fill_value=0)
        group[part_column] = part_no
        return group.reset_index().rename(columns={'index': time_column})
    all_data_daily = daily_sales.groupby(part_column).apply(process_group)
    # ç”±äº groupby().apply() æ–¹æ³•ä¼šåˆ›å»ºä¸€ä¸ªå¤šç´¢å¼• DataFrameï¼Œä½ å¯èƒ½éœ€è¦é‡ç½®ç´¢å¼•
    all_data_daily.reset_index(drop=True, inplace=True)
    # è·å–æ‰€æœ‰é›¶ä»¶çš„ç¼–å·
    part_ids = all_data_daily[part_column].unique()

    # å°†æ‰€æœ‰çš„æ—¥å¸¸æ•°æ®å¤åˆ¶åˆ°ä¸€ä¸ªæ–°çš„å˜é‡ä¸­
    part_data = all_data_daily
    # é€‰æ‹©å°äºç­‰äºè¿™ä¸ªæ—¥æœŸçš„æ•°æ®ä½œä¸ºè®­ç»ƒé›†
    df_train = part_data
    # ä»è®­ç»ƒé›†çš„ DataFrame ä¸­åˆ›å»ºæ—¶é—´åºåˆ— DataFrameï¼Œéœ€è¦æŒ‡å®š ID åˆ—å’Œæ—¶é—´æˆ³åˆ—
    train_data = TimeSeriesDataFrame.from_data_frame(
        df_train,
        id_column=part_column,
        timestamp_column=time_column
        )
    # åˆ›å»ºæ—¶é—´åºåˆ—é¢„æµ‹å™¨ï¼Œéœ€è¦æŒ‡å®šé¢„æµ‹é•¿åº¦ã€ç›®æ ‡å˜é‡åã€è¯„ä¼°æŒ‡æ ‡ç­‰å‚æ•°
    predictor = TimeSeriesPredictor(
            prediction_length=days_to_predict,
            target=sales_column,
            eval_metric="sMAPE",
        )

    # åœ¨è®­ç»ƒæ•°æ®ä¸Šæ‹Ÿåˆé¢„æµ‹å™¨ï¼Œéœ€è¦æŒ‡å®šé¢„è®¾å‚æ•°ã€æ—¶é—´é™åˆ¶ç­‰
    predictor.fit(
            train_data,
            presets="best_quality",
            time_limit=predict_time,
        )

    # åœ¨è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œé¢„æµ‹
    predictions = predictor.predict(train_data)
    return predictions
    

# æ·»åŠ æ ‡é¢˜
st.title('æ±½è½¦é›¶ä»¶é”€é‡é¢„æµ‹')
#æ·»åŠ é¡µé¢è¯´æ˜
st.divider()
with st.chat_message("assistant"):
    st.write("ä½ å¥½ ğŸ‘‹")
    st.write("è¿™æ˜¯ä¸€ä¸ªç”¨äºé¢„æµ‹æ±½è½¦é›¶ä»¶é”€é‡ï¼Œè¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ æ–‡ä»¶ï¼š")
    st.write("1. è¯·ç¡®ä¿æ–‡ä»¶æ ¼å¼ä¸ºcsv")
    st.write("2. è¯·ç¡®ä¿æ–‡ä»¶ä¸­åŒ…å«æ—¶é—´ã€é›¶ä»¶ã€é”€é‡ä¸‰åˆ—")
st.divider()
# åœ¨ä¾§è¾¹æ æ·»åŠ æ–‡ä»¶ä¸Šä¼ å™¨, ç”¨æˆ·å¯ä»¥æ·»åŠ csvæ–‡ä»¶
uploaded_file = st.sidebar.file_uploader('ä¸Šä¼ æ–‡ä»¶', type=['csv'])

# åˆ¤æ–­æ–‡ä»¶æ˜¯å¦ä¸Šä¼ æˆåŠŸ
if uploaded_file is not None:
    df = pd.read_csv(uploaded_file)
    
    with st.spinner('æ­£åœ¨è¯»å–æ•°æ®ï¼š'):
        st.success('æ•°æ®è¯»å–å®Œæˆï¼ä¸‹é¢å±•ç¤ºå‰äº”è¡Œæ•°æ®ï¼š')

    # æ˜¾ç¤ºæ•°æ®å‰äº”è¡Œ
    st.write(df.head())

    # å°†åˆ—åè½¬æ¢ä¸ºå¤§å†™
    df.columns = df.columns.str.upper()

    # æå–æ‰€æœ‰åˆ—
    all_columns = df.columns.tolist()


    st.divider()
    with st.chat_message("assistant"):
        st.write('å¥½çš„ï¼Œè®©æˆ‘ä»¬è¿›è¡Œä¸‹ä¸€æ­¥ï¼')
        st.write('è¯·åœ¨ä¸‹æ–¹é€‰æ‹©æ—¶é—´ã€é›¶ä»¶ã€é”€é‡å¯¹åº”çš„æ•°æ®åˆ—åï¼š')
    st.divider()

    col1, col2, col3 = st.columns(3)

    with col1:
        time_column = st.selectbox('è¯·é€‰æ‹©æ—¶é—´åˆ—', all_columns)
    
    with col2:
        part_column = st.selectbox('è¯·é€‰æ‹©é›¶ä»¶åˆ—', all_columns)
    
    with col3:
        sales_column = st.selectbox('è¯·é€‰æ‹©é”€é‡åˆ—', all_columns)

    # æ˜¾ç¤ºâ€˜åœ¨ä¾§è¾¹æ ä¸­é€‰æ‹©åˆ—åæµè§ˆæ•°æ®â€™
    st.divider()
    with st.chat_message("assistant"):
        st.write('å¥½çš„ï¼Œåœ¨æ‚¨å®Œæˆé€‰æ‹©åè®©æˆ‘ä»¬è¿›è¡Œä¸‹ä¸€æ­¥ï¼')
        st.write('æ‚¨ç°åœ¨å¯ä»¥æŸ¥çœ‹é€‰ä¸­é›¶ä»¶çš„é”€é‡å›¾è¡¨ã€‚')
        st.write('å½“ç„¶ï¼Œæ‚¨ä¹Ÿå¯ä»¥ç›´æ¥é¢„æµ‹é›¶ä»¶çš„é”€é‡ã€‚')
    st.divider()

    if time_column != part_column != sales_column:
        # é€‰æ‹©è¦æ˜¾ç¤ºçš„é›¶ä»¶
        parts_to_show = st.multiselect('è¯·é€‰æ‹©è¦å±•ç¤ºçš„é›¶ä»¶', options=list(df[part_column].unique()))

        # ç»˜åˆ¶é€‰æ‹©çš„é›¶ä»¶çš„é”€é‡è¶‹åŠ¿å›¾
        fig = go.Figure()

        for part in parts_to_show:
            part_df = df[df[part_column] == part]
            fig.add_trace(go.Scatter(x=part_df[time_column], y=part_df[sales_column], mode='lines', name=part))

        fig.update_layout(title='é”€é‡è¶‹åŠ¿å›¾')
        st.plotly_chart(fig)
        st.divider()
        with st.chat_message("assistant"):
            st.write('æ‚¨ç°åœ¨å¯ä»¥å¼€å§‹é¢„æµ‹äº†ï¼')
        st.divider()
        #å¼¹å‡ºâ€œè¯·é€‰æ‹©ä½ è¦é¢„æµ‹çš„é›¶ä»¶â€é€‰æ‹©æ¡†
        part_to_predict = st.multiselect('è¯·é€‰æ‹©è¦é¢„æµ‹çš„é›¶ä»¶', options=list(df[part_column].unique()))
        #å¼¹å‡ºâ€œè¯·é€‰æ‹©ä½ è¦é¢„æµ‹çš„æ—¶é—´é•¿åº¦(å»ºè®®ä¸è¶…è¿‡30å¤©)â€è¾“å…¥æ¡†
        days_to_predict = st.slider('è¯·é€‰æ‹©ä½ è¦é¢„æµ‹çš„æ—¶é—´é•¿åº¦(å»ºè®®ä¸è¶…è¿‡30å¤©)', 0, 60, 7)
        #å¼¹å‡ºâ€œè¯·é€‰æ‹©æ¨¡å‹è¿è¡Œæ—¶é•¿é™åˆ¶(ä¸€èˆ¬è€Œè¨€æ—¶é—´è¶Šé•¿æ•ˆæœè¶Šå¥½ï¼Œé»˜è®¤ä¸º1åˆ†é’Ÿ)â€è¾“å…¥æ¡†
        predict_time = st.slider('è¯·é€‰æ‹©æ¨¡å‹è¿è¡Œæ—¶é•¿é™åˆ¶(å•ä½ï¼šç§’)',  0, 600, 20)
        #å¼¹å‡ºç¡®è®¤æŒ‰é’®
        if st.button('ç¡®è®¤'):
            with st.spinner('æ­£åœ¨è¿›è¡Œæ•°æ®é¢„å¤„ç†ï¼Œå¤„ç†æ–¹æ³•ä¸ºä½¿ç”¨75%åˆ†ä½æ•°å¡«è¡¥è¿‡å¤§å€¼'):
                processed_df = data_process(df, time_column, part_column, sales_column)
                time.sleep(3)
                st.success('æ•°æ®é¢„å¤„ç†å®Œæˆ')
            with st.spinner('æ­£åœ¨é¢„æµ‹ï¼Œè¯·ç¨ç­‰'):
                predictions = main_predict(processed_df, time_column,part_column,sales_column,part_to_predict,days_to_predict,predict_time)
                st.success('é¢„æµ‹å®Œæˆï¼Œé¢„æµ‹ç»“æœå¦‚ä¸‹')
            with st.chat_message("assistant"):
                st.write('é¢„æµ‹ç»“æœä¸ºï¼š')
                for item_id in part_to_predict:
                    y_pred = predictions.loc[item_id]['mean'].sum()
                    #ä¿ç•™ä¸¤ä½å°æ•°
                    y_pred = round(y_pred, 2)
                    #æ‰“å°é¢„æµ‹ç»“æœ
                    st.write('é›¶ä»¶ç¼–å·ä¸º', item_id, 'çš„é›¶ä»¶åœ¨æœªæ¥', days_to_predict, 'å¤©çš„é”€é‡é¢„æµ‹ä¸º', y_pred)
                st.write('é‡æ–°é€‰æ‹©é›¶ä»¶åå¯ä»¥é‡æ–°å¼€å§‹é¢„æµ‹ã€‚')
        
